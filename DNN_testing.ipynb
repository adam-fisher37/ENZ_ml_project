{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN testing playground! \n",
    "\n",
    "i wanna put all the final shit for the DNN into a .py file but for now while I fuck with stuff, ill do it here. that is if this bad boi can even handle the big ass .h5py files I made, if not either make smaller ones or just use alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9246477179882743693\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# boilerplate, ensure you have these packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import callbacks\n",
    "from keras.callbacks import LearningRateScheduler as LRS\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "from keras import optimizers as kopt\n",
    "import keras.backend as K\n",
    "import datetime\n",
    "import h5py\n",
    "import hyperopt as hopt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rc('xtick', direction='in', top=True)\n",
    "mpl.rc('ytick', direction='in', right=True)\n",
    "mpl.rc('xtick.minor', visible=True)\n",
    "mpl.rc('ytick.minor', visible=True)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data rescaling functions, want everything to be [0,1]\n",
    "# rescaling thicknesses, angle of incid, psi, delta\n",
    "# this info should be pulled from the corresponding .txt file!!!\n",
    "# ^although im not quite sure how I wanna do tht rn so just list it in the .txt file\n",
    "def resc_l(thickness):\n",
    "    'rescales the thicknesses to [0,1], takes in arrays'\n",
    "    resc = thickness*1e7 \n",
    "    return resc\n",
    "def resc_ang(aoi):\n",
    "    'rescales the aoi to [0,1], takes in arrays in DEGREES'\n",
    "    resc = aoi/90.\n",
    "    return resc\n",
    "def resc_psi(psi):\n",
    "    '''\n",
    "    rescales ellipsometric coeff psi to basically [0,1], takes in array in DEGREES. returns in rescaled degrees\n",
    "    '''\n",
    "    resc = psi/90.\n",
    "    return resc\n",
    "def resc_del(delta):\n",
    "    '''\n",
    "    rescales ellipsometric coeff delta to be positive (by adding phase shift if needed)\n",
    "    takes in array in RADIANS (b/c its the natural output of tmm.ellips). returns rescaled degrees\n",
    "    '''\n",
    "    if (delta.min()<0.):\n",
    "        resc = delta + np.pi\n",
    "    resc = np.rad2deg(resc)/360.\n",
    "    return resc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funct to read in data\n",
    "def read_dat(fname,dataset,nlay,nang,nwvl):\n",
    "    '''\n",
    "    function to read in the training data within an .h5 file and rescale\n",
    "    currently used for ENZ proj with data for angle, thicknesses, rp, rs, tp, ts, psi, delta\n",
    "    NOTE: data should be a [N,T] array, w/ N for num of runs and T = nang + nlay + 6*nang*nwvl\n",
    "    inputs:\n",
    "    fname - string - file name containing the training data, pls use full file name\n",
    "    dataset - string - name of the dataset in the .h5 file\n",
    "    NOTE: currently doesn't support reading in data from more than 1 dataset\n",
    "    nlay - integer - number of layers in the multistack\n",
    "    nang - integer - number of angles of incidence investigated\n",
    "    nwvl - integer - number of wavelengths investigated\n",
    "    NOTE: will throw errors if inputs are not correct\n",
    "    outputs:\n",
    "    ang - array, float - [N,nang] array of rescaled angles\n",
    "    l - array, float - [N,nlay] array of rescaled thicknesses\n",
    "    rp,rs,tp,ts - array, float - each is a [N,nang*nwvl] array of R and T params\n",
    "    psi, delta - array, float - each is a [N,nang*nwvl] array of ellipsometric coeff in degrees\n",
    "    '''\n",
    "    # assertions to make sure inputs are good\n",
    "    assert isinstance(nang,int), 'incorrect input for: nang'\n",
    "    assert isinstance(nlay,int), 'incorrect input for: nlay'\n",
    "    assert isinstance(nwvl,int), 'incorrect input for: nwvl'\n",
    "    assert isinstance(fname,str), 'incorrect input for: fname'\n",
    "    assert isinstance(dataset,str), 'incorrect input for: dataset'\n",
    "    with h5py.File(fname,'r') as h:\n",
    "        dat = np.array(h.get(dataset))\n",
    "    # assertion to make sure this will read in data correctly\n",
    "    assert (dat[0,0,:].size == nang+nlay+6*nang*nwvl), 'total data shape and input shape do not match'\n",
    "    ang = dat[:,0,0:nang]\n",
    "    l = dat[:,0,nang:(nang+nlay)]\n",
    "    high = nang+nlay+nang*nwvl\n",
    "    rp = dat[:,0,(nang+nlay):high]\n",
    "    low = high\n",
    "    high += nang*nwvl\n",
    "    rs = dat[:,0,low:high]\n",
    "    low = high\n",
    "    high += nang*nwvl\n",
    "    tp = dat[:,0,low:high]\n",
    "    low = high\n",
    "    high += nang*nwvl\n",
    "    ts = dat[:,0,low:high]\n",
    "    low = high\n",
    "    high += nang*nwvl\n",
    "    psi = dat[:,0,low:high]\n",
    "    low = high\n",
    "    high += nang*nwvl\n",
    "    delta = dat[:,0,low:high]\n",
    "    # assertion to make sure data was read in correctly\n",
    "    assert (dat[-1,0,-1]==delta[-1,-1]), 'data was read incorrectly'\n",
    "    # delete the original array that was read in as it is just a space waster now\n",
    "    del dat\n",
    "    l = resc_l(l)\n",
    "    ang = resc_ang(ang)\n",
    "    psi = resc_psi(psi)\n",
    "    # to make sure phase shift is introduced correctly, need to seperate delta by angle\n",
    "    if (nang>1):\n",
    "        for i in range(nang):\n",
    "            delta[:,(i*nwvl):((i+1)*nwvl)] = resc_del(delta[:,(i*nwvl):((i+1)*nwvl)])\n",
    "    delta = resc_del(delta)\n",
    "    return (ang,l,rp,rs,tp,ts,psi,delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split btwn training, validation, and testing datasets\n",
    "# in the future may want this funct to just split the data itself either hardcoded in the funct or just default inputs\n",
    "def split_dat(ang,l,rp,rs,tp,ts,psi,delta,n_tr,n_v,n_te):\n",
    "    '''\n",
    "    function that takes the newly read in data from the .h5 file and splits it into\n",
    "    training, validation, and testing datasets\n",
    "    note: want 90% of the data to be for training and 5% each for validation and testing\n",
    "    note: currently assuming you only have one .h5 file and want to split that btwn the three\n",
    "    inputs:\n",
    "    ang,l,rp,rs,tp,ts,psi,delta - array, float - arrays with the data\n",
    "    n_tr - integer - number of sets for training\n",
    "    n_v - integer - number of sets for validation\n",
    "    n_te - integer - number of sets for testing\n",
    "    outputs:\n",
    "    3x{ang,l,rp,rs,tp,ts,psi,delta} - array, float - arrays with data for each purpose\n",
    "    '''\n",
    "    # make sure you are able to split the data into the amounts you intended\n",
    "    assert (len(ang) == n_tr+n_v+n_te), 'error: number of dataset mismatch'\n",
    "    # training\n",
    "    trang=ang[:n_tr,:]; trl=l[:n_tr,:]; trrp=rp[:n_tr,:]; trrs=rs[:n_tr,:]; trtp=tp[:n_tr,:]; trts=ts[:n_tr,:]; trpsi=psi[:n_tr,:]; trdel=delta[:n_tr,:]\n",
    "    low = n_tr\n",
    "    high = n_tr + n_v\n",
    "    # validation\n",
    "    vang=ang[low:high,:]; vl=l[low:high,:]; vrp=rp[low:high,:]; vrs=rs[low:high,:]; vtp=tp[low:high,:]; vts=ts[low:high,:]; vpsi=psi[low:high,:]; vdel=delta[low:high,:]\n",
    "    low = high\n",
    "    high += n_te\n",
    "    # testing\n",
    "    teang=ang[low:high,:]; tel=l[low:high,:]; terp=rp[low:high,:]; ters=rs[low:high,:]; tetp=tp[low:high,:]; tets=ts[low:high,:]; tepsi=psi[low:high,:]; tedel=delta[low:high,:]\n",
    "    return (trang,trl,trrp,trrs,trtp,trts,trpsi,trdel,vang,vl,vrp,vrs,vtp,vts,vpsi,vdel,teang,tel,terp,ters,tetp,tets,tepsi,tedel)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that combines 'read_dat' and 'split_dat'\n",
    "def rs_dat(fname,dataset,nlay,nang,nwvl,n_tr,n_v,n_te):\n",
    "    '''\n",
    "    function to take care of both 'read_dat' and 'split dat'\n",
    "    and should throw errors if things go wrong\n",
    "    inputs:\n",
    "    fname - string - file name containing the training data, pls use full file name\n",
    "    dataset - string - name of the dataset in the .h5 file\n",
    "    NOTE: currently doesn't support reading in data from more than 1 dataset\n",
    "    nlay - integer - number of layers in the multistack\n",
    "    nang - integer - number of angles of incidence investigated\n",
    "    nwvl - integer - number of wavelengths investigated\n",
    "    n_tr - integer - number of sets for training\n",
    "    n_v - integer - number of sets for validation\n",
    "    n_te - integer - number of sets for testing\n",
    "    outputs:\n",
    "    3x{ang,l,rp,rs,tp,ts,psi,delta} - array, float - arrays with data for each purpose\n",
    "    NOTE: make sure you have enough arrays rdy as output bc this funct deletes redundant data\n",
    "    ^ its 24 (3x8) arrays btw\n",
    "    '''\n",
    "    (a,b,c,d,e,f,g,h) = read_dat(fname,dataset,nlay,nang,nwvl)\n",
    "    (trang,trl,trrp,trrs,trtp,trts,trpsi,trdel,vang,vl,vrp,vrs,vtp,vts,vpsi,vdel,teang,tel,terp,ters,tetp,tets,tepsi,tedel) = split_dat(a,b,c,d,e,f,g,h,n_tr,n_v,n_te)\n",
    "    del a; del b; del c; del d; del e; del f; del g; del h\n",
    "    return (trang,trl,trrp,trrs,trtp,trts,trpsi,trdel,vang,vl,vrp,vrs,vtp,vts,vpsi,vdel,teang,tel,terp,ters,tetp,tets,tepsi,tedel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay function\n",
    "def decay(ep):\n",
    "    'learning rate decay function, input is epochs, may not actually be used since i dont have live update guy'\n",
    "    a = .0025\n",
    "    lr = a/((ep)+1)\n",
    "    return lr\n",
    "lr = LRS(decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heres the model babey\n",
    "def build_mod(drop,nwvl,act='relu'):\n",
    "    '''\n",
    "    builds a DNN model that can be used for machine learing\n",
    "    currently takes the inputs based by angle, does the DNN thing and should be outputting\n",
    "    the thicknesses of each layer in the multistack.\n",
    "    currently assuming there are three aoi's\n",
    "    input:\n",
    "    drop - float - dropout rate can be (0,1) exclusive\n",
    "    nwvl - integer - number of wavelengths investigated\n",
    "    act - string - activation method for Dense layer, default 'relu', see keras Dense documentation for more options\n",
    "    ^ NOTE: this is for the inner Dense layers the ones at the end can/will be different\n",
    "    \n",
    "    output:\n",
    "    model - model obj - a full fledged keras model, it has ONLY BEEN MADE, needs to be compiled and named\n",
    "    '''\n",
    "    # inputs are grouped by aoi\n",
    "    a1 = Input((1,),name='angle_1')\n",
    "    rp1 = Input((nwvl,),name='rp_1'); rs1 = Input((nwvl,),name='rs_1')\n",
    "    tp1 = Input((nwvl,),name='tp_1'); ts1 = Input((nwvl,),name='ts_1')\n",
    "    p1 = Input((nwvl,),name='psi_1'); d1 = Input((nwvl,),name='delta_1')\n",
    "    a2 = Input((1,),name='angle_2')\n",
    "    rp2 = Input((nwvl,),name='rp_2'); rs2 = Input((nwvl,),name='rs_2')\n",
    "    tp2 = Input((nwvl,),name='tp_2'); ts2 = Input((nwvl,),name='ts_2')\n",
    "    p2 = Input((nwvl,),name='psi_2'); d2 = Input((nwvl,),name='delta_2')\n",
    "    a3 = Input((1,),name='angle_3')\n",
    "    rp3 = Input((nwvl,),name='rp_3'); rs3 = Input((nwvl,),name='rs_3')\n",
    "    tp3 = Input((nwvl,),name='tp_3'); ts3 = Input((nwvl,),name='ts_3')\n",
    "    p3 = Input((nwvl,),name='psi_3'); d3 = Input((nwvl,),name='delta_3')\n",
    "    # connect the inputs\n",
    "    # currently im thinking connecting them in a similar fasion as the rows\n",
    "    # these are midpoints and so shall be named: m##, 1st #: layer num, 2nd #: index within layer\n",
    "    # note: add layer makes a tensor of the same size of the inputs but idk how tht works if they arent the same size so im just doing it like andy for rn\n",
    "    m11 = Add()([rp1,rs1,a1]); m12 = Add()([tp1,ts1,a1]); m13 = Add()([p1,d1,a1])\n",
    "    m14 = Add()([rp2,rs2,a2]); m15 = Add()([tp2,ts2,a2]); m16 = Add()([p2,d2,a2])\n",
    "    m17 = Add()([rp3,rs3,a3]); m18 = Add()([tp3,ts3,a3]); m19 = Add()([p3,d3,a3])\n",
    "    # dense layer, followed by a dropout\n",
    "    # name scheme: x##: 1st #: dense layer num, 2nd #: index within layer\n",
    "    x11 = Dense(nwvl,activation=act)(m11); x11 = Dropout(drop)(x11)\n",
    "    x12 = Dense(nwvl,activation=act)(m12); x12 = Dropout(drop)(x12)\n",
    "    x13 = Dense(nwvl,activation=act)(m13); x13 = Dropout(drop)(x13)\n",
    "    x14 = Dense(nwvl,activation=act)(m14); x14 = Dropout(drop)(x14)\n",
    "    x15 = Dense(nwvl,activation=act)(m15); x15 = Dropout(drop)(x15)\n",
    "    x16 = Dense(nwvl,activation=act)(m16); x16 = Dropout(drop)(x16)\n",
    "    x17 = Dense(nwvl,activation=act)(m17); x17 = Dropout(drop)(x17)\n",
    "    x18 = Dense(nwvl,activation=act)(m18); x18 = Dropout(drop)(x18)\n",
    "    x19 = Dense(nwvl,activation=act)(m19); x19 = Dropout(drop)(x19)\n",
    "    # another Add layer, now m2#, combining all data from each aoi together\n",
    "    m21 = Add()([x11,x12,x13])\n",
    "    m22 = Add()([x14,x15,x16])\n",
    "    m23 = Add()([x17,x18,x19])\n",
    "    # another Dense layer, also followed by a dropout \n",
    "    # name now x2#, also increase output size\n",
    "    x21 = Dense((nwvl+50),activation=act)(m21); x21 = Dropout(drop)(x21)\n",
    "    x22 = Dense((nwvl+50),activation=act)(m22); x22 = Dropout(drop)(x22)\n",
    "    x23 = Dense((nwvl+50),activation=act)(m23); x23 = Dropout(drop)(x23)\n",
    "    # another Add layer, now m3#, combining all data together\n",
    "    m31 = Add()([x21,x22,x23])\n",
    "    # another dense layer with dropout after, now x3#\n",
    "    x31 = Dense((nwvl+100),activation=act)(m31)\n",
    "    x31 = Dropout(drop)(x31)\n",
    "    # last 'internal' layer, prep for output layer\n",
    "    # note: i have no idea what the output for this layer should be, consult andy\n",
    "    x41 = Dense(100,activation=act)(x31)\n",
    "    # output layer\n",
    "    # note: no idea what the activation should be for this one, consult andy\n",
    "    l_out = Dense(15,activation=None)(x41)\n",
    "    # put it all together and compile\n",
    "    model = Model([a1,rp1,rs1,tp1,ts1,p1,d1,a2,rp2,rs2,tp2,ts2,p2,d2,a3,rp3,rs3,tp3,ts3,p3,d3],[l_out])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop = 0.\n",
    "wvl = np.linspace(400.,1000.,121)\n",
    "nwvl = len(wvl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = build_mod(drop,nwvl)\n",
    "# compiling the same way as andy rn\n",
    "mod.compile(optimizer='adam',loss=['mse'],metrics=['mse'],loss_weights=[10])\n",
    "modname = 'enzchar_invdes_model_d21621.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\afish\\.conda\\envs\\ml_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\afish\\.conda\\envs\\ml_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\afish\\.conda\\envs\\ml_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\afish\\.conda\\envs\\ml_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\afish\\.conda\\envs\\ml_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\afish\\.conda\\envs\\ml_env\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod.save(modname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you beautiful son of a bitch, naming the input layers means you can pass in a dictionary!!\n",
    "# do model stuff here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will parallelize with 1 cores.\n",
      "\n",
      "doing that data thang, please wait...\n",
      "puttin that mf in ya file\n",
      "congrats u done!\n"
     ]
    }
   ],
   "source": [
    "# testing all the shit\n",
    "import run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('traindata_enzchar_lay15_mats3_n0.5k_20210216.hdf5','r') as h:\n",
    "    dat = np.array(h.get('data'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2196)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat[:,0,:].shape\n",
    "# 3+15+6*3*121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trang,trl,trrp,trrs,trtp,trts,trpsi,trdel,vang,vl,vrp,vrs,vtp,vts,vpsi,vdel,teang,tel,terp,ters,tetp,tets,tepsi,tedel) = rs_dat('traindata_enzchar_lay15_mats3_n0.5k_20210216.hdf5','data',15,3,nwvl,400,50,50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
